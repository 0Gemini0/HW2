{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, id2df = index.get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_document = index.document_base()\n",
    "last_document = index.maximum_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164597\n"
     ]
    }
   ],
   "source": [
    "print(index.document_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airbus Subsidies\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "print((queries['51']))\n",
    "for query_id in queries:\n",
    "    query_tokens = index.tokenize(queries[query_id])\n",
    "    query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "    query.extend([word_id for word_id in query_id_tokens if word_id > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456\n",
      "1\n",
      "164598\n"
     ]
    }
   ],
   "source": [
    "print(len(set(query)))\n",
    "print(index.document_base())\n",
    "print(index.maximum_document())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_base(): 1\n",
      "maximum_document(): 164598\n",
      "document(doc_id): ('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412))\n",
      "document_count(): 164597\n",
      "document_length(doc_id) 521\n",
      "ext_document_id(doc_id) AP890425-0001\n",
      "get_dictionary()[0], term:term_id {'grammatically': 104586, 'prl': 191960, 'abilitites': 17190}\n",
      "get_dictionary()[1], term_id:term {1: 'new', 2: 'percent', 3: 'two'}\n",
      "get_dictionary()[2], term_id:doc_frequency {1: 76807, 2: 40665, 3: 79449}\n",
      "get_term_frequencies(), term_id:term_frequency {1: 171350, 2: 153174, 3: 143637}\n",
      "index_path index/\n",
      "process_term(word) \n",
      "query(string) ((100285, -10.274313356655172), (2707, -10.377165157778187))\n",
      "term_count(word), term_frequency 171350\n",
      "tokenize(string) ['', '', 'nice']\n",
      "total_terms(), total number of terms: 75983583\n",
      "unique_terms(), total number of unique terms: 267318\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# starting and maximum index of documents, id's are ordered consequentially\n",
    "print('document_base():', index.document_base()) # minimum\n",
    "print('maximum_document():', index.maximum_document()) # maximum\n",
    "\n",
    "# return tuple of (external_document_name, (term_ids list))\n",
    "doc_id = 1\n",
    "document = index.document(doc_id)\n",
    "doc_name = document[0]\n",
    "doc_terms = document[1]\n",
    "# to save space only the first 10 terms in the doc are printed\n",
    "print('document(doc_id):', (doc_name,doc_terms[:10]))\n",
    "print('document_count():', index.document_count())\n",
    "print('document_length(doc_id)', index.document_length(doc_id))\n",
    "print('ext_document_id(doc_id)', index.ext_document_id(doc_id))\n",
    "\n",
    "def subsample_dictionary(dictionary, item_count):    \n",
    "    dict_new = {}\n",
    "    for key in list(dictionary)[:item_count]:\n",
    "        dict_new[key] = dictionary[key]\n",
    "    return dict_new\n",
    "token2id, id2token, id2df = index.get_dictionary()\n",
    "dict_new = subsample_dictionary(token2id, 3)\n",
    "print('get_dictionary()[0], term:term_id', dict_new)\n",
    "dict_new = subsample_dictionary(id2token, 3)\n",
    "print('get_dictionary()[1], term_id:term', dict_new)\n",
    "dict_new = subsample_dictionary(id2df, 3)\n",
    "print('get_dictionary()[2], term_id:doc_frequency', dict_new)\n",
    "print('get_term_frequencies(), term_id:term_frequency', subsample_dictionary(index.get_term_frequencies(), 3))\n",
    "print('index_path', index.path)\n",
    "print('process_term(word)', index.process_term('very'))\n",
    "print('query(string)', index.query('happy new year')[:2])\n",
    "print('term_count(word), term_frequency', index.term_count('new'))\n",
    "print('tokenize(string)', index.tokenize('this is nice'))\n",
    "print('total_terms(), total number of terms:', index.total_terms())\n",
    "print('unique_terms(), total number of unique terms:', index.unique_terms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docFreq = {}\n",
    "# collFreq = {}\n",
    "# termFreq = []\n",
    "# start = time.time()\n",
    "# for q in query:\n",
    "#     collFreq[q] = 0\n",
    "#     docFreq[q] = 0\n",
    "\n",
    "# for docID in range(index.document_base(),index.maximum_document()):\n",
    "#     doc = (index.document(docID)[0],{})\n",
    "#     for q in query:\n",
    "#         freq = 0\n",
    "#         for word in index.document(docID)[1]:\n",
    "#             if word == q:\n",
    "# #                 print(q, word)\n",
    "#                 freq += 1\n",
    "# #                 break\n",
    "#         collFreq[q] += freq\n",
    "#         docFreq[q] += 1\n",
    "#     termFreq.append(doc)\n",
    "# #     break    \n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.014456510543823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(docs,open('docs.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.172136545181274\n"
     ]
    }
   ],
   "source": [
    "docFreq = {}\n",
    "collFreq = {}\n",
    "docs = []\n",
    "\n",
    "for q in query:\n",
    "    collFreq[q] = 0\n",
    "    docFreq[q] = 0\n",
    "    \n",
    "for docID in range(index.document_base(),index.maximum_document()):\n",
    "    docs.append(index.document(docID))\n",
    "    \n",
    "termFreq = {}\n",
    "uniqWords = {}\n",
    "\n",
    "# for q in query:\n",
    "#     termFreq[q] = {}\n",
    "start = time.time()\n",
    "for docID in range(last_document-1):\n",
    "#     print(docID)\n",
    "    tf_doc = Counter(docs[docID][1])\n",
    "    tf_doc = dict(tf_doc)\n",
    "#     print(len(tf_doc.keys()))\n",
    "    temp = {}\n",
    "    temp = {q:tf_doc[q] for q in query if q in tf_doc.keys()}\n",
    "#         v = tf_doc.keys()\n",
    "#         if q in v:\n",
    "#             temp[q] = tf_doc[q]\n",
    "#             collFreq[q] += freq\n",
    "#             docFreq[q] += 1\n",
    "    termFreq[docs[docID][0]] = temp\n",
    "    uniqWords[docs[docID][0]] = len(tf_doc.keys())-1\n",
    "#     print(termFreq[docs[docID][0]])\n",
    "#     print(uniqWords[docs[docID][0]])\n",
    "#     print(docs[docID][0])\n",
    "#     print(temp)\n",
    "#     break\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{985: 1, 75: 2, 235: 1, 57: 2, 1329: 1}\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "print(termFreq[docs[100500][0]])\n",
    "# dict_you_want = { your_key: old_dict[your_key] for your_key in your_keys }\n",
    "print((uniqWords[docs[100000][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tf_doc = Counter(index.document(2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.89999999999999\n"
     ]
    }
   ],
   "source": [
    "# # print(type(dict(tf_doc)))\n",
    "# print(dict(tf_doc))\n",
    "# khash = dict(tf_doc)\n",
    "# for q in query:\n",
    "#     v = khash.keys()\n",
    "#     if q in v:\n",
    "#         print(q)\n",
    "print(0.00065*166000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164597\n",
      "{80: 3, 1: 10, 258: 1, 75: 1, 1812: 1, 373: 3, 2294: 1, 1033: 1, 38: 3, 236: 1}\n"
     ]
    }
   ],
   "source": [
    "print(len(termFreq))\n",
    "print(termFreq[docs[164596][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
